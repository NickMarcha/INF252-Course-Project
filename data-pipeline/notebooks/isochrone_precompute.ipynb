{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Isochrone Precomputation: Oslo Bysykkel\n",
        "\n",
        "Precomputes isochrone polygons for each station from trip data:\n",
        "1. Per-connection (station pair) outlier filtering via IQR\n",
        "2. Travel time matrix (median duration per pair; haversine estimate for missing)\n",
        "3. Per-station interpolation onto grid + contour extraction\n",
        "4. Export to prepared-data/isochrones.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "07c799c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\n",
            "Raw data: c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\raw-data\n",
            "Prepared data: c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\n",
            "--- Last execution ---\n",
            "  Timestamp: 2026-02-18T00:00:00\n",
            "  Duration:  0s\n",
            "  System:    ?\n",
            "  CPUs:      ? | Processor: ?\n",
            "  RAM:       N/A\n",
            "------------------------\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SETUP: Project paths and execution utils\n",
        "# =============================================================================\n",
        "from pathlib import Path\n",
        "import json\n",
        "import sys\n",
        "\n",
        "cwd = Path.cwd()\n",
        "project_root = cwd if (cwd / \"package.json\").exists() else cwd.parent.parent\n",
        "raw_dir = project_root / \"raw-data\"\n",
        "prepared_dir = project_root / \"prepared-data\"\n",
        "\n",
        "sys.path.insert(0, str(project_root / \"data-pipeline\"))\n",
        "from execution_utils import show_execution_banner, write_with_execution_metadata\n",
        "\n",
        "print(\"Project root:\", project_root)\n",
        "print(\"Raw data:\", raw_dir)\n",
        "print(\"Prepared data:\", prepared_dir)\n",
        "\n",
        "out_path = prepared_dir / \"isochrones.json\"\n",
        "_pipeline_start_time = show_execution_banner(out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "20399c7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORTS\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20c4d9fb",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "012d3c64",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 9761398 trips (excluding same-station)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Load trip data from raw-data/YYYY/MM.json\n",
        "# =============================================================================\n",
        "records = []\n",
        "for year_dir in sorted(raw_dir.iterdir()):\n",
        "    if not year_dir.is_dir():\n",
        "        continue\n",
        "    year = int(year_dir.name)\n",
        "    for json_path in sorted(year_dir.glob(\"*.json\")):\n",
        "        month = int(json_path.stem)\n",
        "        with open(json_path, encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        trips = data if isinstance(data, list) else data.get(\"data\", data.get(\"trips\", []))\n",
        "        for t in trips:\n",
        "            records.append((year, month, t))\n",
        "\n",
        "rows = []\n",
        "for year, month, t in records:\n",
        "    rows.append({\n",
        "        \"year\": year,\n",
        "        \"month\": month,\n",
        "        \"duration\": t.get(\"duration\"),\n",
        "        \"start_station_id\": str(t.get(\"start_station_id\")),\n",
        "        \"start_station_name\": t.get(\"start_station_name\"),\n",
        "        \"end_station_id\": str(t.get(\"end_station_id\")),\n",
        "        \"end_station_name\": t.get(\"end_station_name\"),\n",
        "        \"start_lat\": t.get(\"start_station_latitude\"),\n",
        "        \"start_lon\": t.get(\"start_station_longitude\"),\n",
        "        \"end_lat\": t.get(\"end_station_latitude\"),\n",
        "        \"end_lon\": t.get(\"end_station_longitude\"),\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df = df.dropna(subset=[\"duration\", \"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\"])\n",
        "df = df[df[\"start_station_id\"] != df[\"end_station_id\"]].copy()\n",
        "\n",
        "print(f\"Loaded {len(df)} trips (excluding same-station)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67d3d30d",
      "metadata": {},
      "source": [
        "## Phase 1: Per-Connection Outlier Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0984dd2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After per-pair IQR filter: 8863239 trips (from 9761398)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Per (start_station_id, end_station_id): IQR outlier removal\n",
        "# =============================================================================\n",
        "MIN_TRIPS_FOR_IQR = 5\n",
        "IQR_MULTIPLIER = 1.5\n",
        "\n",
        "def iqr_mask(g):\n",
        "    if len(g) < MIN_TRIPS_FOR_IQR:\n",
        "        return pd.Series(True, index=g.index)\n",
        "    q1 = g.quantile(0.25)\n",
        "    q3 = g.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    if iqr == 0:\n",
        "        return pd.Series(True, index=g.index)\n",
        "    low = q1 - IQR_MULTIPLIER * iqr\n",
        "    high = q3 + IQR_MULTIPLIER * iqr\n",
        "    return (g >= low) & (g <= high)\n",
        "\n",
        "mask = df.groupby([\"start_station_id\", \"end_station_id\"])[\"duration\"].transform(iqr_mask)\n",
        "df_filtered = df[mask].copy()\n",
        "\n",
        "print(f\"After per-pair IQR filter: {len(df_filtered)} trips (from {len(df)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d29209b",
      "metadata": {},
      "source": [
        "## Phase 2: Travel Time Matrix & Station Catalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "477b85f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stations: 292\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Build station catalog (id, name, lat, lon) from first occurrence\n",
        "# =============================================================================\n",
        "stations_start = df_filtered.groupby(\"start_station_id\").agg({\n",
        "    \"start_station_name\": \"first\",\n",
        "    \"start_lat\": \"first\",\n",
        "    \"start_lon\": \"first\",\n",
        "}).rename(columns={\"start_station_name\": \"name\", \"start_lat\": \"lat\", \"start_lon\": \"lon\"})\n",
        "stations_end = df_filtered.groupby(\"end_station_id\").agg({\n",
        "    \"end_station_name\": \"first\",\n",
        "    \"end_lat\": \"first\",\n",
        "    \"end_lon\": \"first\",\n",
        "}).rename(columns={\"end_station_name\": \"name\", \"end_lat\": \"lat\", \"end_lon\": \"lon\"})\n",
        "\n",
        "all_ids = set(stations_start.index) | set(stations_end.index)\n",
        "stations = []\n",
        "for sid in sorted(all_ids, key=lambda x: (len(str(x)), x)):\n",
        "    if sid in stations_start.index:\n",
        "        row = stations_start.loc[sid]\n",
        "    else:\n",
        "        row = stations_end.loc[sid]\n",
        "    stations.append({\"id\": sid, \"name\": str(row[\"name\"]), \"lat\": float(row[\"lat\"]), \"lon\": float(row[\"lon\"])})\n",
        "\n",
        "stations_df = pd.DataFrame(stations)\n",
        "print(f\"Stations: {len(stations)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8087d7f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Median speed (km/s): 0.002725\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Travel time matrix: median duration per (origin, dest) in seconds\n",
        "# =============================================================================\n",
        "tt_matrix = (\n",
        "    df_filtered.groupby([\"start_station_id\", \"end_station_id\"])[\"duration\"]\n",
        "    .median()\n",
        "    .reset_index()\n",
        ")\n",
        "tt_matrix.columns = [\"origin\", \"dest\", \"duration_sec\"]\n",
        "\n",
        "# Median speed from filtered trips (haversine distance / duration)\n",
        "def haversine_km(lat1, lon1, lat2, lon2):\n",
        "    R = 6371\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "    return R * 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "df_filtered[\"dist_km\"] = haversine_km(\n",
        "    df_filtered[\"start_lat\"], df_filtered[\"start_lon\"],\n",
        "    df_filtered[\"end_lat\"], df_filtered[\"end_lon\"]\n",
        ")\n",
        "df_filtered[\"speed_km_per_sec\"] = df_filtered[\"dist_km\"] / df_filtered[\"duration\"]\n",
        "median_speed = df_filtered[\"speed_km_per_sec\"].median()\n",
        "print(f\"Median speed (km/s): {median_speed:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3a9c8480",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# For each origin station: get travel times to all dests (observed or estimated)\n",
        "# =============================================================================\n",
        "def get_travel_times_from_origin(origin_id):\n",
        "    origin_row = stations_df[stations_df[\"id\"] == origin_id].iloc[0]\n",
        "    o_lat, o_lon = origin_row[\"lat\"], origin_row[\"lon\"]\n",
        "    results = []\n",
        "    for _, dest_row in stations_df.iterrows():\n",
        "        if dest_row[\"id\"] == origin_id:\n",
        "            results.append((dest_row[\"lat\"], dest_row[\"lon\"], 0.0))\n",
        "            continue\n",
        "        match = tt_matrix[(tt_matrix[\"origin\"] == origin_id) & (tt_matrix[\"dest\"] == dest_row[\"id\"])]\n",
        "        if len(match) > 0:\n",
        "            dur_min = match.iloc[0][\"duration_sec\"] / 60.0\n",
        "        else:\n",
        "            d_km = haversine_km(o_lat, o_lon, dest_row[\"lat\"], dest_row[\"lon\"])\n",
        "            dur_min = (d_km / median_speed) / 60.0\n",
        "        results.append((dest_row[\"lat\"], dest_row[\"lon\"], dur_min))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "860db533",
      "metadata": {},
      "source": [
        "## Phase 3: Interpolate & Extract Contours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8c23c0a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Grid bounds (Oslo) and resolution\n",
        "# =============================================================================\n",
        "LAT_MIN = 59.898\n",
        "LAT_MAX = 59.955\n",
        "LON_MIN = 10.648\n",
        "LON_MAX = 10.818\n",
        "GRID_N = 100\n",
        "TIME_BANDS_MIN = [5, 10, 15, 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8e2d0dc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Contour to GeoJSON polygon (manual extraction from matplotlib)\n",
        "# =============================================================================\n",
        "def contour_to_geojson_polygon(lon_2d, lat_2d, z, level):\n",
        "    \"\"\"Extract contour at level, return GeoJSON Polygon or None.\"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    cs = ax.contour(lon_2d, lat_2d, z, levels=[level])\n",
        "    plt.close(fig)\n",
        "    paths = cs.get_paths()\n",
        "    if len(paths) == 0:\n",
        "        return None\n",
        "    if len(paths) == 0:\n",
        "        return None\n",
        "    # Take largest path (main boundary)\n",
        "    path = max(paths, key=lambda p: len(p.vertices))\n",
        "    verts = path.vertices\n",
        "    coords = [[float(x), float(y)] for x, y in verts]\n",
        "    if coords and (coords[0][0] != coords[-1][0] or coords[0][1] != coords[-1][1]):\n",
        "        coords.append(coords[0])\n",
        "    return {\"type\": \"Polygon\", \"coordinates\": [coords]}\n",
        "\n",
        "def compute_isochrones_for_station(origin_id):\n",
        "    points = get_travel_times_from_origin(origin_id)\n",
        "    lats = np.array([p[0] for p in points])\n",
        "    lons = np.array([p[1] for p in points])\n",
        "    values = np.array([p[2] for p in points])\n",
        "    lon_1d = np.linspace(LON_MIN, LON_MAX, GRID_N)\n",
        "    lat_1d = np.linspace(LAT_MIN, LAT_MAX, GRID_N)\n",
        "    lon_2d, lat_2d = np.meshgrid(lon_1d, lat_1d)\n",
        "    zi = griddata((lons, lats), values, (lon_2d, lat_2d), method=\"linear\", fill_value=60.0)\n",
        "    polygons = {}\n",
        "    for t in TIME_BANDS_MIN:\n",
        "        poly = contour_to_geojson_polygon(lon_2d, lat_2d, zi, t)\n",
        "        if poly is not None:\n",
        "            polygons[str(t)] = poly\n",
        "    return polygons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1b31f4fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing station 1/292: Tøyenparken\n",
            "Processing station 50/292: Briskeby\n",
            "Processing station 100/292: Jernbanetorget\n",
            "Processing station 150/292: Bryn T-Bane\n",
            "Processing station 200/292: Henrik Wergelands allé\n",
            "Processing station 250/292: Bygdøy allé\n",
            "Done. Computed isochrones for 292 stations.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Compute isochrones for all stations\n",
        "# =============================================================================\n",
        "isochrones = {}\n",
        "for i, row in enumerate(stations_df.itertuples()):\n",
        "    if (i + 1) % 50 == 0 or i == 0:\n",
        "        print(f\"Processing station {i+1}/{len(stations_df)}: {row.name}\")\n",
        "    isochrones[row.id] = compute_isochrones_for_station(row.id)\n",
        "\n",
        "print(f\"Done. Computed isochrones for {len(isochrones)} stations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\isochrones.json\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Export to prepared-data/isochrones.json\n",
        "# =============================================================================\n",
        "export_data = {\n",
        "    \"stations\": stations,\n",
        "    \"time_bands_min\": TIME_BANDS_MIN,\n",
        "    \"isochrones\": isochrones,\n",
        "}\n",
        "\n",
        "write_with_execution_metadata(out_path, export_data, _pipeline_start_time)\n",
        "print(f\"Wrote {out_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "inf252-data-pipeline",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Station Trip Counts per Hour\n",
        "\n",
        "Computes trip counts per hour (0–23) for each station and route from raw trip data, binned by day, month, and year. Station counts split into incoming (destination) and outgoing (origin).\n",
        "\n",
        "**Outputs:**\n",
        "- `prepared-data/stations/station_totals.parquet` – station_id, in, out\n",
        "- `prepared-data/stations/station_by_year.parquet` – period, station_id, direction, hour, count\n",
        "- `prepared-data/stations/station_by_month.parquet` – period, station_id, direction, hour, count\n",
        "- `prepared-data/stations/station_by_day_YYYY.parquet` – by_day (yearly Parquet files)\n",
        "- `prepared-data/stations/station_trip_counts_meta.json` – last_execution metadata\n",
        "- `prepared-data/routes/route_pair_counts.parquet` – flat totals (route_key, count)\n",
        "- `prepared-data/routes/route_by_year.parquet` – year-binned (period, route_key, hour, count)\n",
        "- `prepared-data/routes/route_by_month.parquet` – month-binned (period, route_key, hour, count)\n",
        "- `prepared-data/routes/route_by_day_YYYY.parquet` – by_day (yearly Parquet files)\n",
        "- `prepared-data/routes/route_trip_counts_meta.json` – last_execution metadata\n",
        "\n",
        "Data stays separate from routes for use in various frontend visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No previous execution info (file does not exist yet).\n",
            "Project root: c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\n",
            "Raw data: c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\raw-data\n",
            "Output: c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SETUP: Paths, execution banner\n",
        "# =============================================================================\n",
        "from pathlib import Path\n",
        "import json\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "\n",
        "cwd = Path.cwd()\n",
        "project_root = cwd if (cwd / \"package.json\").exists() else cwd.parent.parent\n",
        "raw_dir = project_root / \"raw-data\"\n",
        "prepared_dir = project_root / \"prepared-data\"\n",
        "stations_dir = prepared_dir / \"stations\"\n",
        "routes_dir = prepared_dir / \"routes\"\n",
        "stations_dir.mkdir(parents=True, exist_ok=True)\n",
        "routes_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sys.path.insert(0, str(project_root / \"data-pipeline\"))\n",
        "from execution_utils import show_execution_banner, write_with_execution_metadata\n",
        "\n",
        "out_path = stations_dir / \"station_trip_counts_meta.json\"\n",
        "_pipeline_start_time = show_execution_banner(out_path)\n",
        "\n",
        "print(\"Project root:\", project_root)\n",
        "print(\"Raw data:\", raw_dir)\n",
        "print(\"Output:\", stations_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load raw trips and compute counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10,034,294 trips\n",
            "Stations: 8 years, 80 months, 2398 days\n",
            "Routes: 80805 pairs, 8 years\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Load raw trips, count per station per hour (in/out split), binned by day/month/year\n",
        "# Station: period -> station_id -> \"in\"|\"out\" -> hour -> count\n",
        "# Route: period -> route_key -> hour -> count, plus flat pair_counts\n",
        "# =============================================================================\n",
        "station_by_year = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(int))))\n",
        "station_by_month = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(int))))\n",
        "station_by_day = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(int))))\n",
        "station_totals = defaultdict(lambda: {\"in\": 0, \"out\": 0})\n",
        "route_by_year = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "route_by_month = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "route_by_day = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "pair_counts = defaultdict(int)  # flat totals: \"origin_id|dest_id\" -> count\n",
        "\n",
        "if not raw_dir.exists():\n",
        "    raise FileNotFoundError(\"raw-data/ not found. Run npm run download first.\")\n",
        "\n",
        "total_trips = 0\n",
        "for year_dir in sorted(raw_dir.iterdir()):\n",
        "    if not year_dir.is_dir():\n",
        "        continue\n",
        "    year_str = year_dir.name\n",
        "    for json_path in sorted(year_dir.glob(\"*.json\")):\n",
        "        with open(json_path, encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        trips = data if isinstance(data, list) else data.get(\"data\", data.get(\"trips\", []))\n",
        "        for t in trips:\n",
        "            started_at = t.get(\"started_at\")\n",
        "            if not started_at:\n",
        "                continue\n",
        "            try:\n",
        "                dt = datetime.fromisoformat(started_at.replace(\"Z\", \"+00:00\"))\n",
        "            except (ValueError, TypeError):\n",
        "                continue\n",
        "            hour = dt.hour\n",
        "            year = str(dt.year)\n",
        "            month = f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "            day = f\"{dt.year:04d}-{dt.month:02d}-{dt.day:02d}\"\n",
        "            start_id = str(t.get(\"start_station_id\", \"\")).strip()\n",
        "            end_id = str(t.get(\"end_station_id\", \"\")).strip()\n",
        "            if start_id:\n",
        "                station_by_year[year][start_id][\"out\"][hour] += 1\n",
        "                station_by_month[month][start_id][\"out\"][hour] += 1\n",
        "                station_by_day[day][start_id][\"out\"][hour] += 1\n",
        "                station_totals[start_id][\"out\"] += 1\n",
        "            if end_id and end_id != start_id:\n",
        "                station_by_year[year][end_id][\"in\"][hour] += 1\n",
        "                station_by_month[month][end_id][\"in\"][hour] += 1\n",
        "                station_by_day[day][end_id][\"in\"][hour] += 1\n",
        "                station_totals[end_id][\"in\"] += 1\n",
        "            if start_id and end_id and start_id != end_id:\n",
        "                route_key = f\"{start_id}|{end_id}\"\n",
        "                route_by_year[year][route_key][hour] += 1\n",
        "                route_by_month[month][route_key][hour] += 1\n",
        "                route_by_day[day][route_key][hour] += 1\n",
        "                pair_counts[route_key] += 1\n",
        "            total_trips += 1\n",
        "\n",
        "print(f\"Processed {total_trips:,} trips\")\n",
        "print(f\"Stations: {len(station_by_year)} years, {len(station_by_month)} months, {len(station_by_day)} days\")\n",
        "print(f\"Routes: {len(pair_counts)} pairs, {len(route_by_year)} years\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prune zeros and write output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed legacy stations/station_trip_counts.json\n",
            "Removed legacy stations/station_trip_counts_by_day_2019.json\n",
            "Removed legacy stations/station_trip_counts_by_day_2020.json\n",
            "Removed legacy stations/station_trip_counts_by_day_2021.json\n",
            "Removed legacy stations/station_trip_counts_by_day_2022.json\n",
            "Removed legacy stations/station_trip_counts_by_day_2023.json\n",
            "Removed legacy stations/station_trip_counts_by_day_2024.json\n",
            "Removed legacy stations/station_trip_counts_by_day_2025.json\n",
            "Removed legacy stations/station_trip_counts_by_day_2026.json\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_totals.parquet (292 stations)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_year.parquet (82156 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_month.parquet (718694 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_day_2019.parquet (1346816 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_day_2020.parquet (1273812 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_day_2021.parquet (1308566 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_day_2022.parquet (1174712 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_day_2023.parquet (1071185 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_day_2024.parquet (1097688 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_day_2025.parquet (1104033 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_by_day_2026.parquet (15445 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\stations\\station_trip_counts_meta.json\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_pair_counts.parquet (80805 routes)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_year.parquet (3011899 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_month.parquet (6218231 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_day_2019.parquet (1968048 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_day_2020.parquet (1519250 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_day_2021.parquet (1299461 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_day_2022.parquet (1157041 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_day_2023.parquet (1010088 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_day_2024.parquet (1037669 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_day_2025.parquet (1011728 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_by_day_2026.parquet (8865 rows)\n",
            "Wrote c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\prepared-data\\routes\\route_trip_counts_meta.json\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Prune zeros (omit keys with count 0), convert to JSON-serializable dict\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "\n",
        "def prune_zeros(nested: dict) -> dict:\n",
        "    \"\"\"For routes: period -> route_key -> hour -> count. Omit zero hours.\"\"\"\n",
        "    result = {}\n",
        "    for period, entities in nested.items():\n",
        "        result[period] = {}\n",
        "        for entity_id, hours in entities.items():\n",
        "            pruned = {str(h): c for h, c in hours.items() if c > 0}\n",
        "            if pruned:\n",
        "                result[period][entity_id] = pruned\n",
        "    return result\n",
        "\n",
        "\n",
        "def prune_station_zeros(nested: dict) -> dict:\n",
        "    \"\"\"For stations: period -> station_id -> \"in\"|\"out\" -> hour -> count. Omit zeros.\"\"\"\n",
        "    result = {}\n",
        "    for period, stations in nested.items():\n",
        "        result[period] = {}\n",
        "        for station_id, dirs in stations.items():\n",
        "            pruned_in = {str(h): c for h, c in dirs.get(\"in\", {}).items() if c > 0}\n",
        "            pruned_out = {str(h): c for h, c in dirs.get(\"out\", {}).items() if c > 0}\n",
        "            if pruned_in or pruned_out:\n",
        "                result[period][station_id] = {}\n",
        "                if pruned_in:\n",
        "                    result[period][station_id][\"in\"] = pruned_in\n",
        "                if pruned_out:\n",
        "                    result[period][station_id][\"out\"] = pruned_out\n",
        "    return result\n",
        "\n",
        "\n",
        "def prune_totals(totals: dict) -> dict:\n",
        "    \"\"\"Station totals: {sid: {\"in\": x, \"out\": y}}. Omit if both zero.\"\"\"\n",
        "    return {\n",
        "        sid: {\"in\": d[\"in\"], \"out\": d[\"out\"]}\n",
        "        for sid, d in totals.items()\n",
        "        if d[\"in\"] or d[\"out\"]\n",
        "    }\n",
        "\n",
        "\n",
        "def flatten_route_binned(nested: dict) -> pd.DataFrame:\n",
        "    \"\"\"Flatten route binned dict (period -> route_key -> hour -> count) to DataFrame for Parquet.\"\"\"\n",
        "    rows = []\n",
        "    for period, routes in nested.items():\n",
        "        for route_key, hours in routes.items():\n",
        "            for hour, count in hours.items():\n",
        "                if count > 0:\n",
        "                    rows.append({\"period\": period, \"route_key\": route_key, \"hour\": int(hour), \"count\": count})\n",
        "    return pd.DataFrame(rows) if rows else pd.DataFrame(columns=[\"period\", \"route_key\", \"hour\", \"count\"])\n",
        "\n",
        "\n",
        "def flatten_station_binned(nested: dict) -> pd.DataFrame:\n",
        "    \"\"\"Flatten station binned dict (period -> station_id -> in/out -> hour -> count) to DataFrame for Parquet.\"\"\"\n",
        "    rows = []\n",
        "    for period, stations in nested.items():\n",
        "        for station_id, dirs in stations.items():\n",
        "            for direction, hours in dirs.items():\n",
        "                for hour, count in hours.items():\n",
        "                    if count > 0:\n",
        "                        rows.append({\"period\": period, \"station_id\": station_id, \"direction\": direction, \"hour\": int(hour), \"count\": count})\n",
        "    return pd.DataFrame(rows) if rows else pd.DataFrame(columns=[\"period\", \"station_id\", \"direction\", \"hour\", \"count\"])\n",
        "\n",
        "\n",
        "# Clean up legacy files at prepared-data root and in subfolders\n",
        "for old in list(prepared_dir.glob(\"station_trip_counts*.json\")) + list(prepared_dir.glob(\"route_trip_counts*.json\")):\n",
        "    if old.is_file():\n",
        "        old.unlink()\n",
        "        print(f\"Removed legacy {old.relative_to(prepared_dir)}\")\n",
        "for old in list(stations_dir.glob(\"station_trip_counts*.json\")):\n",
        "    if old.is_file():\n",
        "        old.unlink()\n",
        "        print(f\"Removed legacy stations/{old.name}\")\n",
        "\n",
        "\n",
        "# Station outputs (Parquet format to stations/)\n",
        "totals_pruned = prune_totals(station_totals)\n",
        "totals_df = pd.DataFrame([{\"station_id\": sid, \"in\": d[\"in\"], \"out\": d[\"out\"]} for sid, d in totals_pruned.items()])\n",
        "totals_df.to_parquet(stations_dir / \"station_totals.parquet\", index=False)\n",
        "print(f\"Wrote {stations_dir / 'station_totals.parquet'} ({len(totals_df)} stations)\")\n",
        "\n",
        "station_by_year_pruned = prune_station_zeros(station_by_year)\n",
        "df_year = flatten_station_binned(station_by_year_pruned)\n",
        "df_year.to_parquet(stations_dir / \"station_by_year.parquet\", index=False)\n",
        "print(f\"Wrote {stations_dir / 'station_by_year.parquet'} ({len(df_year)} rows)\")\n",
        "\n",
        "station_by_month_pruned = prune_station_zeros(station_by_month)\n",
        "df_month = flatten_station_binned(station_by_month_pruned)\n",
        "df_month.to_parquet(stations_dir / \"station_by_month.parquet\", index=False)\n",
        "print(f\"Wrote {stations_dir / 'station_by_month.parquet'} ({len(df_month)} rows)\")\n",
        "\n",
        "station_by_day_pruned = prune_station_zeros(station_by_day)\n",
        "station_years = sorted({day[:4] for day in station_by_day_pruned})\n",
        "for year in station_years:\n",
        "    year_data = {day: station_by_day_pruned[day] for day in station_by_day_pruned if day.startswith(year)}\n",
        "    df_day = flatten_station_binned(year_data)\n",
        "    year_path = stations_dir / f\"station_by_day_{year}.parquet\"\n",
        "    df_day.to_parquet(year_path, index=False)\n",
        "    print(f\"Wrote {year_path} ({len(df_day)} rows)\")\n",
        "\n",
        "station_meta_path = stations_dir / \"station_trip_counts_meta.json\"\n",
        "write_with_execution_metadata(station_meta_path, {}, _pipeline_start_time)\n",
        "print(f\"Wrote {station_meta_path}\")\n",
        "\n",
        "# Route outputs (Parquet format to routes/)\n",
        "# Remove legacy JSON files if present\n",
        "for old in [routes_dir / \"route_trip_counts.json\"] + list(routes_dir.glob(\"route_trip_counts_by_day_*.json\")):\n",
        "    if old.exists():\n",
        "        old.unlink()\n",
        "        print(f\"Removed legacy {old.name}\")\n",
        "\n",
        "pair_df = pd.DataFrame([{\"route_key\": k, \"count\": v} for k, v in pair_counts.items() if v > 0])\n",
        "pair_df.to_parquet(routes_dir / \"route_pair_counts.parquet\", index=False)\n",
        "print(f\"Wrote {routes_dir / 'route_pair_counts.parquet'} ({len(pair_df)} routes)\")\n",
        "\n",
        "df_year = flatten_route_binned(prune_zeros(route_by_year))\n",
        "df_year.to_parquet(routes_dir / \"route_by_year.parquet\", index=False)\n",
        "print(f\"Wrote {routes_dir / 'route_by_year.parquet'} ({len(df_year)} rows)\")\n",
        "\n",
        "df_month = flatten_route_binned(prune_zeros(route_by_month))\n",
        "df_month.to_parquet(routes_dir / \"route_by_month.parquet\", index=False)\n",
        "print(f\"Wrote {routes_dir / 'route_by_month.parquet'} ({len(df_month)} rows)\")\n",
        "\n",
        "route_by_day_pruned = prune_zeros(route_by_day)\n",
        "route_years = sorted({day[:4] for day in route_by_day_pruned})\n",
        "for year in route_years:\n",
        "    year_data = {day: route_by_day_pruned[day] for day in route_by_day_pruned if day.startswith(year)}\n",
        "    df_day = flatten_route_binned(year_data)\n",
        "    year_path = routes_dir / f\"route_by_day_{year}.parquet\"\n",
        "    df_day.to_parquet(year_path, index=False)\n",
        "    print(f\"Wrote {year_path} ({len(df_day)} rows)\")\n",
        "\n",
        "# Route metadata (last_execution only)\n",
        "route_meta_path = routes_dir / \"route_trip_counts_meta.json\"\n",
        "write_with_execution_metadata(route_meta_path, {}, _pipeline_start_time)\n",
        "print(f\"Wrote {route_meta_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "inf252-data-pipeline",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

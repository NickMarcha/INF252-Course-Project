{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Routes API: Single-Route Test\n",
        "\n",
        "Tests one bicycle route fetch to validate API key, request format, and response.\n",
        "Uses `routes_fetch.py` for cache-before-fetch and force-fetch support."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b3f4972a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\n",
            "Cache dir: c:\\Users\\Nicol\\Desktop\\INF252-Course-Project\\routes-cache\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SETUP: Paths, load .env, get API key\n",
        "# =============================================================================\n",
        "from pathlib import Path\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "cwd = Path.cwd()\n",
        "project_root = cwd if (cwd / \"package.json\").exists() else cwd.parent.parent\n",
        "prepared_dir = project_root / \"prepared-data\"\n",
        "cache_dir = project_root / \"routes-cache\"\n",
        "\n",
        "sys.path.insert(0, str(project_root / \"data-pipeline\"))\n",
        "from routes_fetch import fetch_route, fetch_routes_batch\n",
        "\n",
        "api_key = os.environ.get(\"GOOGLE_ROUTES_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\n",
        "        \"Set GOOGLE_ROUTES_API_KEY in environment or .env file. \"\n",
        "        \"Copy .env.example to .env and add your key.\"\n",
        "    )\n",
        "\n",
        "print(\"Project root:\", project_root)\n",
        "print(\"Cache dir:\", cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ecd1f0a",
      "metadata": {},
      "source": [
        "## Load Stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "255ef69b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 292 stations from stations.json\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Load stations from stations.json or isochrones.json (fallback)\n",
        "# =============================================================================\n",
        "stations_path = prepared_dir / \"stations.json\"\n",
        "if not stations_path.exists():\n",
        "    stations_path = prepared_dir / \"isochrones.json\"\n",
        "\n",
        "if not stations_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"No stations file found. Run stations_prepare.ipynb first, \"\n",
        "        f\"or ensure isochrones.json exists in {prepared_dir}\"\n",
        "    )\n",
        "\n",
        "with open(stations_path, encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "stations = data.get(\"data\", data).get(\"stations\", data.get(\"stations\", []))\n",
        "print(f\"Loaded {len(stations)} stations from {stations_path.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce3c60c",
      "metadata": {},
      "source": [
        "## Fetch Single Route"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8272e0f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success (cached=True)\n",
            "  Duration: 184s\n",
            "  Distance: 1282 m\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Fetch route: Tøyenparken (377) -> Grønlands torg (381)\n",
        "# Set FORCE_ROUTES_FETCH=1 in env to bypass cache during development.\n",
        "# =============================================================================\n",
        "origin_id = \"377\"\n",
        "dest_id = \"381\"\n",
        "\n",
        "result = fetch_route(origin_id, dest_id, stations, api_key, cache_dir)\n",
        "\n",
        "cached = result.get(\"cached\", False)\n",
        "resp = result.get(\"response\", {})\n",
        "routes = resp.get(\"routes\", [])\n",
        "\n",
        "if routes:\n",
        "    r = routes[0]\n",
        "    duration = r.get(\"duration\", \"N/A\")\n",
        "    distance_m = r.get(\"distanceMeters\", \"N/A\")\n",
        "    print(f\"Success (cached={cached})\")\n",
        "    print(f\"  Duration: {duration}\")\n",
        "    print(f\"  Distance: {distance_m} m\")\n",
        "else:\n",
        "    print(f\"No route returned. Response: {resp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f3d3d8",
      "metadata": {},
      "source": [
        "## Fetch Batch: Top N Stations + Top K Connections (~1000 requests)\n",
        "\n",
        "1. Top N stations by total_trips (incoming + outgoing)\n",
        "2. For each station, top K connections from trip data (pairs where that station is origin or dest)\n",
        "3. Fetch all unique pairs in both directions (target ~1000; only uncached trigger API calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5c4d2444",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 50 stations by total_trips:\n",
            "  421: Alexander Kiellands Plass (263,345 trips)\n",
            "  551: Olaf Ryes plass (259,527 trips)\n",
            "  489: Torggata (257,818 trips)\n",
            "  398: Ringnes Park (249,088 trips)\n",
            "  480: Helga Helgesens plass (245,850 trips)\n",
            "  443: Sjøsiden øst (231,478 trips)\n",
            "  479: Tjuvholmen (222,466 trips)\n",
            "  464: Sukkerbiten (220,405 trips)\n",
            "  396: Kirkeristen (215,740 trips)\n",
            "  408: Tøyen skole (196,753 trips)\n",
            "  446: Bislett Stadion (176,568 trips)\n",
            "  478: Jernbanetorget (174,143 trips)\n",
            "  384: Vår Frelsers gravlund sør (167,986 trips)\n",
            "  494: Rådhusbrygge 4 (167,848 trips)\n",
            "  465: Bjørvika (166,401 trips)\n",
            "  460: Botanisk Hage sør (162,591 trips)\n",
            "  424: Birkelunden (160,605 trips)\n",
            "  423: Schous plass (158,326 trips)\n",
            "  507: Jens Bjelkes gate (157,514 trips)\n",
            "  412: Jakob kirke (153,994 trips)\n",
            "  437: Sentrum Scene (153,270 trips)\n",
            "  493: Sofienbergparken nordvest (148,467 trips)\n",
            "  444: AHO (148,180 trips)\n",
            "  413: Majorstuen (146,477 trips)\n",
            "  598: Sofienbergparken nord (137,559 trips)\n",
            "  521: Jess Carlsens gate (137,531 trips)\n",
            "  607: Marcus Thranes gate (137,062 trips)\n",
            "  495: Vaterlandsparken (137,026 trips)\n",
            "  620: Bislettgata (136,806 trips)\n",
            "  611: Bankplassen (135,877 trips)\n",
            "  579: Bogstadveien (134,043 trips)\n",
            "  442: Vulkan (133,244 trips)\n",
            "  537: St. Olavs gate (132,080 trips)\n",
            "  390: Saga Kino (132,020 trips)\n",
            "  737: Munkegata trikkestopp (130,713 trips)\n",
            "  447: Kværnerbyen (130,478 trips)\n",
            "  1755: Aker Brygge (127,893 trips)\n",
            "  499: Bjerregaards gate (124,667 trips)\n",
            "  599: Paléhaven (124,169 trips)\n",
            "  503: Fagerheimgata (123,862 trips)\n",
            "  597: Fredensborg (121,406 trips)\n",
            "  430: Spikersuppa Vest (121,191 trips)\n",
            "  534: Filipstadveien (117,471 trips)\n",
            "  526: Lille Grensen (114,090 trips)\n",
            "  557: Akersgata (114,065 trips)\n",
            "  381: Grønlands torg (113,953 trips)\n",
            "  484: Karenlyst allé (112,177 trips)\n",
            "  440: Lakkegata (111,505 trips)\n",
            "  452: Vippetangen vest (111,040 trips)\n",
            "  485: Sommerfrydhagen (110,465 trips)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 1: Top N stations by total_trips (incoming + outgoing)\n",
        "# =============================================================================\n",
        "TOP_N = 50\n",
        "\n",
        "stations_sorted = sorted(\n",
        "    stations,\n",
        "    key=lambda s: int(s.get(\"total_trips\", 0)),\n",
        "    reverse=True,\n",
        ")\n",
        "top_stations = stations_sorted[:TOP_N]\n",
        "top_ids = {str(s[\"id\"]) for s in top_stations}\n",
        "station_by_id = {str(s[\"id\"]): s for s in stations}\n",
        "\n",
        "print(f\"Top {TOP_N} stations by total_trips:\")\n",
        "for s in top_stations:\n",
        "    print(f\"  {s['id']}: {s['name']} ({s.get('total_trips', 0):,} trips)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c19756e4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total observed (origin, dest) pairs in trip data: 80,805\n",
            "Unique pairs from top 20 connections per station: 745\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 2: For each top-N station, find top K connections from trip data\n",
        "# =============================================================================\n",
        "raw_dir = project_root / \"raw-data\"\n",
        "CONNECTIONS_PER_STATION = 20\n",
        "\n",
        "# Count (origin, dest) pairs from trip data\n",
        "pair_counts = {}\n",
        "if raw_dir.exists():\n",
        "    for year_dir in sorted(raw_dir.iterdir()):\n",
        "        if not year_dir.is_dir():\n",
        "            continue\n",
        "        for json_path in sorted(year_dir.glob(\"*.json\")):\n",
        "            with open(json_path, encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "            trips = data if isinstance(data, list) else data.get(\"data\", data.get(\"trips\", []))\n",
        "            for t in trips:\n",
        "                oid = str(t.get(\"start_station_id\", \"\"))\n",
        "                did = str(t.get(\"end_station_id\", \"\"))\n",
        "                if not oid or not did or oid == did:\n",
        "                    continue\n",
        "                pair_counts[(oid, did)] = pair_counts.get((oid, did), 0) + 1\n",
        "else:\n",
        "    raise FileNotFoundError(\"raw-data/ not found. Run npm run download first.\")\n",
        "\n",
        "print(f\"Total observed (origin, dest) pairs in trip data: {len(pair_counts):,}\")\n",
        "\n",
        "# For each top-N station, get top K pairs (where station is origin or dest)\n",
        "# Only keep pairs where both stations exist in our stations data\n",
        "unique_pairs = set()\n",
        "for sid in top_ids:\n",
        "    candidates = [\n",
        "        (p, c) for p, c in pair_counts.items()\n",
        "        if (p[0] == sid or p[1] == sid)\n",
        "        and p[0] in station_by_id and p[1] in station_by_id\n",
        "    ]\n",
        "    candidates.sort(key=lambda x: -x[1])\n",
        "    for (oid, did), _ in candidates[:CONNECTIONS_PER_STATION]:\n",
        "        unique_pairs.add((oid, did))\n",
        "\n",
        "print(f\"Unique pairs from top {CONNECTIONS_PER_STATION} connections per station: {len(unique_pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0d2fcba8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total to process: 1000 (capped at 1000)\n",
            "Already cached: 65 → ~935 new API calls\n",
            "Done: fetched 935, used cache 65\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 3: Fetch both directions per pair, cap at ~1000 (only uncached hit API)\n",
        "# =============================================================================\n",
        "TARGET_FETCHES = 1000\n",
        "\n",
        "fetch_list = []\n",
        "for (oid, did) in unique_pairs:\n",
        "    fetch_list.append((oid, did))\n",
        "    fetch_list.append((did, oid))\n",
        "\n",
        "# Deduplicate (in case (A,B) and (B,A) both came from top connections)\n",
        "seen = set()\n",
        "deduped = []\n",
        "for oid, did in fetch_list:\n",
        "    key = (oid, did)\n",
        "    if key not in seen:\n",
        "        seen.add(key)\n",
        "        deduped.append((oid, did))\n",
        "\n",
        "# Sort by frequency (most-traveled routes first), cap at TARGET_FETCHES\n",
        "def route_freq(p):\n",
        "    oid, did = p\n",
        "    return pair_counts.get((oid, did), 0) + pair_counts.get((did, oid), 0)\n",
        "\n",
        "deduped.sort(key=route_freq, reverse=True)\n",
        "fetch_list = deduped[:TARGET_FETCHES]\n",
        "\n",
        "# Count how many of our fetch_list are already cached (no API call for these)\n",
        "single_dir = cache_dir / \"single\"\n",
        "cached_in_list = (\n",
        "    sum(1 for (oid, did) in fetch_list if (single_dir / f\"{oid}_{did}.json\").exists())\n",
        "    if single_dir.exists() else 0\n",
        ")\n",
        "print(f\"Total to process: {len(fetch_list)} (capped at {TARGET_FETCHES})\")\n",
        "print(f\"Already cached: {cached_in_list} → ~{len(fetch_list) - cached_in_list} new API calls\")\n",
        "\n",
        "results = fetch_routes_batch(\n",
        "    fetch_list,\n",
        "    stations,\n",
        "    api_key,\n",
        "    cache_dir,\n",
        ")\n",
        "cached_count = sum(1 for r in results if r.get(\"cached\"))\n",
        "fetched_count = len(results) - cached_count\n",
        "print(f\"Done: fetched {fetched_count}, used cache {cached_count}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "inf252-data-pipeline",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
